# @package _global_

defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

# Directory for project output files
project_dir: "./output"

# Name of the environment to use
environment: "commonroad" # Either "commonroad" or "image_cartpole"

# Random seed for reproducibility
seed: 123

# Automatically select the device (CPU/GPU)
device: "auto"

# Set to false to disable debug output
debug_mode: false

# Whether to log outputs to terminal
verbose: false

# Dataset generation options
dataset:
  # Number of episodes to collect for the dataset
  num_episodes: 100
  # Number of processes for dataset generation
  num_workers: 1
  # Number of episodes per batch
  storage_batch_size: 1024 
  # Number of timesteps for observation
  t_obs: 10
  # Number of timesteps for prediction
  t_pred: 10
  # Number of frames to skip between each recorded observation frame
  obs_skip_frames: 0
  # Number of frames to skip between each recorded prediction frame
  pred_skip_frames: 4
  # Whether to use polar transform for observations
  use_polar_transform: true
  # Whether to preprocess an existing dataset
  preprocess_existing: true
  # Number of worker processes for preprocessing (0 for single-process)
  preprocess_workers: 0

wandb:
  # Enable Weights & Biases logging
  enabled: true
  # Project name for Weights & Biases
  project: PredictiveStateRepresentations-AD

representation:
  # Type of model to use for encoding
  model_type: "PredictiveModelV9M1" # AutoEncoderModelV0, AutoEncoderModelV1, PredictiveModelV8, PredictiveModelV9, PredictiveModelV9M1, VQVAEPredictiveModel
  # Path to the model
  model_path: "../models/pretrained/predictive_model_v9m1.pth"

models:
  VQVAEPredictiveModel:
    pretrained_model_path: "../models/pretrained/autoencoder_model_v0.pth"
  PredictiveModelV9:
    pretrained_model_path: "../models/pretrained/autoencoder_model_v0.pth"
  AutoEncoderModelV0:
    kl_weight: 0.001
  PredictiveModelV9M1:
    encoder_learning_rate: 5e-6
    decoder_learning_rate: 2e-5
  PredictiveModelV9M2:
    pretrained_model_path: "../models/pretrained/autoencoder_model_v0m1.pth"
    decoder_learning_rate: 2e-5
    vib_weight: 0.0001  # Weight for the Variational Information Bottleneck KL loss
    vae_weight: 0.00001  # Weight for the variational decoding KL loss
    condition_on_future_actions: false

# Training configuration
training:
  # Type of model to use for training
  model_type: "PredictiveModelV9M1" # AutoEncoderModelV0, AutoEncoderModelV0M1, AutoEncoderModelV1, PredictiveModelV8, PredictiveModelV9, PredictiveModelV9M1, PredictiveModelV9M2, VQVAEPredictiveModel
  # Set this to the path of a checkpoint to resume training
  warmstart_model: null # "../models/pretrained/predictive_model_v9m1.pth"
  # Whether to continue from the last scheduler state
  warmstart_load_scheduler_state: false
  # Latent representation size
  hidden_dim: 64
  # Number of training epochs
  epochs: 100000
  # Number of batches to use per epoch (set to null to use all batches)
  batches_per_epoch: 1
  # Batch size for training
  batch_size: 1
  # Minibatch size
  minibatch_size: 16
  # Number of training iterations per batch
  iterations_per_batch: 4
  # Whether to compute and print a validation error at each minibatch (slow)
  track_val_loss: false
  # Factor to downsample input images
  downsample_factor: 1
  # Maximum gradient norm for gradient clipping
  max_grad_norm: 2.0
  # Set to true to enable automatic mixed precision
  use_amp: true 
  # Validation set split size (if float, it is the ratio of the dataset, if int, it is the number of samples)
  val_size: 1
  # Whether to use pinned host memory for faster data copies
  pin_memory: true
  # Number of processes for fetching data in dataloader
  num_workers: 0
  # Prefetch factor for dataloader. This is the number of batches to load in advance. Higher values can speed up training.
  prefetch_factor: null
  # Whether to plot predictions during training
  create_plots: true
  # Whether to log training metrics to stdout
  stdout_logging: true
  # Interval for saving model checkpoints
  save_interval: 10
  # Whether to overwrite existing model checkpoints
  overwrite_checkpoints: true
  # Loss function to use for training
  loss:
    mse_weight: 1.0
    l1_weight: 0.0
    diversity_weight: 0.0
    latent_l1_weight: 0.005
    latent_l2_weight: 0.005
    temporal_decay: 0.9
    perceptual_weight: 0.0
    num_scales: 3
    use_sample_weights: false
    r_weight: 1.0
    g_weight: 1.0
    b_weight: 1.0

  # Optimizer options
  optimizer:
    type: "AdamW"
    beta1: 0.9
    beta2: 0.999
    learning_rate: 1e-4
    weight_decay: 1e-5
    momentum: 0.9
    alpha: 0.99

  # Scheduler configuration
  scheduler:
    type: "NoScheduler"  # Options: "StepLR", "ExponentialLR", "CosineAnnealingLR", "ReduceLROnPlateau", "NoScheduler"
    T_max: 100  # Period of cosine annealing in epochs
    eta_min: 1e-5  # Minimum learning rate
    # Other scheduler options (uncomment if needed):
    # step_size: 30  # Only used for StepLR
    # gamma: 0.1  # Used for StepLR and ExponentialLR
    # factor: 0.5  # Only used for ReduceLROnPlateau
    # patience: 10  # Only used for ReduceLROnPlateau
    # threshold: 1e-4  # Only used for ReduceLROnPlateau

# Reinforcement Learning Training options
rl_training:
  # Warmstart model for training
  warmstart_model: null # "./rl/models/final_model.zip", "latest" # Set to "latest" to use the latest model
  # Path to save trained models
  save_path: "./rl/models"
  # Path to save training logs
  log_path: "./rl/logs"
  # Frequency of evaluation during training
  eval_freq: 10000
  # Number of episodes for evaluation
  n_eval_episodes: 20
  # Frequency of video recording (in steps)
  video_freq: 30000  
  # Length of each video (in steps)
  video_length: 2000 
  # Folder to save videos (relative to project_dir)
  video_folder: "./rl/videos"  
  # Number of parallel environments for training
  num_envs: 16
  # Total number of timesteps for training
  total_timesteps: 10000000
  # Learning rate for the optimizer
  learning_rate: 5e-5
  # Number of steps to collect samples for each update
  n_steps: 1024
  # Batch size for each update
  batch_size: 256
  # Number of epochs to train the model
  n_epochs: 4
  # Discount factor for future rewards
  gamma: 0.99
  # Lambda parameter for Generalized Advantage Estimation
  gae_lambda: 0.8
  # Clipping range for the surrogate objective
  clip_range: 0.15
  # Coefficient for the entropy loss
  ent_coef: 0.03
  # Coefficient for the value function loss
  vf_coef: 0.5
  # Maximum norm for gradient clipping
  max_grad_norm: 10.0
  # Network architecture for the policy and value function
  net_arch:
    pi: [256, 256, 128]
    vf: [256, 256, 128]

# Viewer options
viewer:
  # Range of view in the simulation
  view_range: 65.0
  # Size of the viewer window
  window_size: 64

# Environment-specific options
commonroad: 
  scenario_dir: "scenarios"

  # Simulation options
  simulation:
    # Steps for lanelet graph conversion (if any)
    lanelet_graph_conversion_steps: null
    # Use linear projection for lanelets
    linear_lanelet_projection: true
    # Whether to sort lanelet assignments
    sort_lanelet_assignments: false

  # Control space options
  control_space:
    # Minimum acceleration
    lower_bound_acceleration: -6.0
    # Maximum acceleration
    upper_bound_acceleration: 6.5
    # Minimum velocity
    lower_bound_velocity: 0.001
    # Minimum steering angle
    lower_bound_steering: -0.15
    # Maximum steering angle
    upper_bound_steering: 0.15
    # Minimum velocity for steering
    min_velocity_steering: 1.0

  # Respawner options
  respawner:
    # Randomize initial arclength
    random_init_arclength: true
    # Randomize goal arclength
    random_goal_arclength: true
    # Randomize start timestep
    random_start_timestep: true
    # Spawn only at intersections
    only_intersections: false
    # Min and max route length
    route_length: [10, 15]
    # Initial speed distribution
    init_speed: "uniform_random"
    # Minimum initial arclength
    min_init_arclength: 50.0
    # Range for random speed
    random_speed_range: [0.0, 50.0]
    # Initial steering angle distribution
    init_steering_angle: "normal_random"
    # Range for random steering angle
    random_steering_angle_range: [-0.08, 0.08]
    # Params for normal distribution for steering angle
    normal_steering_angle_params: [0.0, 0.02]
    # Noise in initial orientation
    init_orientation_noise: 0.4
    # Noise in initial position
    init_position_noise: 3.0
    # Minimum distance to goal
    min_goal_distance: 100.0
    # Minimum L2 distance to goal
    min_goal_distance_l2: 100.0
    # Maximum L2 distance to goal (if any)
    max_goal_distance_l2: null
    # Maximum distance to goal (if any)
    max_goal_distance: null
    # Minimum remaining distance (if any)
    min_remaining_distance: null
    # Maximum outer attempts for respawning
    max_attempts_outer: 50
    # Minimum distance between vehicles
    min_vehicle_distance: 0.0
    # Minimum vehicle speed (if any)
    min_vehicle_speed: null
    # Minimum number of vehicles on route (if any)
    min_vehicles_route: null
    # Maximum inner attempts for respawning
    max_attempts_inner: 5

  # Traffic extraction options
  traffic_extraction:
    # Type of edge drawer to use
    edge_drawer: "NoEdgeDrawer"
    # List of postprocessors to apply
    postprocessors: []
    # Consider only ego-centric incoming edges
    only_ego_inc_edges: false
    # Assign vehicles to multiple lanelets
    assign_multiple_lanelets: true
    # Radius of ego-centric map
    ego_map_radius: 50.0
    # Include lanelet vertices in extraction
    include_lanelet_vertices: false

image_cartpole:
  max_episode_steps: 500
